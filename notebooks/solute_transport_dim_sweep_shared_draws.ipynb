{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2578c82c",
   "metadata": {},
   "source": [
    "# Toy Model for Solute Transport : Dimension and M Sweep (Shared Draws)\n",
    "\n",
    "This notebook runs uniform MESS (multiple M) and MH across dimensions, using a shared draw at d_max for the prior sample and observation noise, then subsets for each d.\n",
    "\n",
    "Notes:\n",
    "- Chains are saved under estimations/AD_toy_dim_M_sweep_shared_draws.\n",
    "- a_true, theta_true, and observation noise come from the same d_max draw and are subset for each d.\n",
    "- Set d_max >= max(d_list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c9eda",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(repo_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from mess.problems.advection_diffusion import (\n",
    "    make_omegas_power,\n",
    "    make_Astar_nn,\n",
    "    make_Astar_from_atrue,\n",
    "    params_from_skew,\n",
    "    prior_diag_from_powerlaw,\n",
    "    solve_theta,\n",
    " )\n",
    "from mess.problems.advection_diffusion import AdvectionDiffusionToy\n",
    "from mess.algorithms.mess import mess_step\n",
    "from mess.algorithms.mh import mh_chain\n",
    "from mess.algorithms.effective_sample_size import estimate_effective_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151cad0",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f204ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep configuration\n",
    "seed_data = 0\n",
    "seed_mcmc = 0\n",
    "n_iters = 30000\n",
    "burn_in = 10000\n",
    "thin = 1\n",
    "max_lag = 1500\n",
    "\n",
    "d_list = [10] #[10, 15, 20, 25, 30, 35, 40, 45, 50] \n",
    "d_max = 100\n",
    "M_list = [10] #[1, 10, 50, 100] #, 100, 200]\n",
    "\n",
    "# Data hyperparameters\n",
    "kappa = 0.02\n",
    "sigma = 0.5\n",
    "alpha = 3\n",
    "gamma = 2\n",
    "tau2 = 2.0\n",
    "a_mode = 'nearest_neighbor'\n",
    "use_prior_A = True\n",
    "shared_draws_seed = seed_data\n",
    "\n",
    "if max(d_list) > d_max:\n",
    "    raise ValueError('d_max must be >= max(d_list)')\n",
    "\n",
    "# Observation configuration\n",
    "obs_highest_freq = 6\n",
    "obs_bandwidth = 3\n",
    "obs_config = \"central_modes\"\n",
    "\n",
    "# MH proposal covariance (\"isotropic\" or \"prior\").\n",
    "mh_proposal_cov = \"prior\"\n",
    "mh_proposal_isotropic_std = 0.000018\n",
    "mh_proposal_prior_std = 0.105\n",
    "if mh_proposal_cov == \"isotropic\":\n",
    "    mh_proposal_std_chosen = mh_proposal_isotropic_std\n",
    "elif mh_proposal_cov == \"prior\":\n",
    "    mh_proposal_std_chosen = mh_proposal_prior_std\n",
    "\n",
    "# MH proposal stds per d (tuned to get 23.4% for d=20).\n",
    "mh_proposal_stds_scaled = np.repeat(mh_proposal_std_chosen, len(d_list))\n",
    "if len(mh_proposal_stds_scaled) != len(d_list):\n",
    "    raise ValueError('mh_proposal_stds_scaled must match d_list length')\n",
    "\n",
    "\n",
    "# Run flags (default False to avoid long runs).\n",
    "run_mess = True\n",
    "run_mh = False\n",
    "recompute_corrupt_chains = True\n",
    "\n",
    "# Parallelization (dim-level)\n",
    "use_parallel = True\n",
    "max_workers = 1  # Keep small to avoid overloading the machine.\n",
    "\n",
    "# Cache generated datasets per dimension.\n",
    "datasets_by_dim = {}\n",
    "\n",
    "# Output directory under estimations/\n",
    "run_tag = (\n",
    "    f\"priorA{use_prior_A}_obs_{obs_config}_tau2{tau2}_sigma{sigma}_seed{seed_data}_\"\n",
    "    f\"dmax{d_max}_Niters{n_iters}\"\n",
    " )\n",
    "output_dir = Path(repo_root) / 'estimations' / \"AD_toy_dim_M_sweep_shared_draws\" / run_tag\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reports directory for figures, tables, and metrics.\n",
    "reports_dir = Path(repo_root) / 'reports' / \"AD_toy_dim_M_sweep_shared_draws\" / run_tag\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Output dir:', output_dir)\n",
    "print('Reports dir:', reports_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cab94",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_msjd_per_param(chain):\n",
    "    if chain.shape[0] < 2:\n",
    "        return np.zeros(chain.shape[1])\n",
    "    jumps = np.diff(chain, axis=0)\n",
    "    msjd = np.mean(jumps * jumps, axis=0)\n",
    "    return msjd\n",
    "\n",
    "def compute_ess_per_param(chain, max_lag):\n",
    "    if chain.shape[0] < 2:\n",
    "        return np.zeros(chain.shape[1])\n",
    "    variances = np.var(chain, axis=0)\n",
    "    if np.all(variances == 0):\n",
    "        return np.zeros(chain.shape[1])\n",
    "    ess_vals = estimate_effective_sample_size(chain, max_lag=max_lag)\n",
    "    ess_vals = np.asarray(ess_vals, dtype=float)\n",
    "    ess_vals[variances == 0] = 0.0\n",
    "    return ess_vals\n",
    "\n",
    "def chain_path(output_dir, d, alg, M=None, proposal_std=None, proposal_cov=None):\n",
    "    alg_key = alg.lower()\n",
    "    if alg_key == 'mh':\n",
    "        if proposal_std is None:\n",
    "            return output_dir / f'chain_d{d}_mh_sigma2unknown.npz'\n",
    "        sigma_tag = f'{proposal_std:.6g}'\n",
    "        cov_tag = '' if proposal_cov in (None, 'isotropic') else f'_cov{proposal_cov}'\n",
    "        return output_dir / f'chain_d{d}_mh_sigma2{sigma_tag}{cov_tag}.npz'\n",
    "    return output_dir / f'chain_d{d}_{alg_key}_M{M}.npz'\n",
    "\n",
    "def load_chain(output_dir, d, alg, M=None, proposal_std=None, proposal_cov=None):\n",
    "    path = chain_path(output_dir, d, alg, M, proposal_std=proposal_std, proposal_cov=proposal_cov)\n",
    "    if not path.exists() and alg.lower() == 'mh':\n",
    "        if proposal_std is None:\n",
    "            matches = sorted(output_dir.glob(f'chain_d{d}_mh_sigma2*.npz'))\n",
    "        else:\n",
    "            sigma_tag = f'{proposal_std:.6g}'\n",
    "            matches = sorted(output_dir.glob(f'chain_d{d}_mh_sigma2{sigma_tag}*.npz'))\n",
    "        if matches:\n",
    "            path = matches[0]\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        data = np.load(path)\n",
    "        return data['chain']\n",
    "    except (zipfile.BadZipFile, ValueError, KeyError) as exc:\n",
    "        print(f'Corrupt or unreadable chain file: {path.name} ({exc})')\n",
    "        return None\n",
    "\n",
    "def is_chain_readable(path):\n",
    "    if not path.exists():\n",
    "        return False\n",
    "    try:\n",
    "        with np.load(path) as data:\n",
    "            _ = data['chain']\n",
    "        return True\n",
    "    except (zipfile.BadZipFile, ValueError, KeyError):\n",
    "        return False\n",
    "\n",
    "def save_chain(path, chain, metadata):\n",
    "    np.savez_compressed(path, chain=chain, **metadata)\n",
    "\n",
    "def get_obs_indices(dim_value, highest_freq, bandwidth):\n",
    "    highest_freq = min(highest_freq, dim_value)\n",
    "    bandwidth = min(bandwidth, dim_value)\n",
    "    start = max(0, highest_freq - bandwidth + 1)\n",
    "    return np.arange(start, highest_freq + 1, dtype=int)\n",
    "\n",
    "def get_param_indices_for_dim(dim, shared_draws):\n",
    "    cache = shared_draws.setdefault('param_indices_cache', {})\n",
    "    if dim not in cache:\n",
    "        iju = shared_draws['param_iju']\n",
    "        mask = (iju[0] < dim) & (iju[1] < dim)\n",
    "        cache[dim] = np.nonzero(mask)[0]\n",
    "    return cache[dim]\n",
    "\n",
    "def build_shared_draws(\n",
    "    d_max,\n",
    "    kappa,\n",
    "    sigma,\n",
    "    alpha,\n",
    "    gamma,\n",
    "    tau2,\n",
    "    offset,\n",
    "    a_mode,\n",
    "    seed,\n",
    " ):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    m_max = d_max * (d_max - 1) // 2\n",
    "    prior_diag_max = prior_diag_from_powerlaw(\n",
    "        d_max, alpha=alpha, gamma=gamma, tau2=tau2, offset=offset\n",
    "    )\n",
    "    if prior_diag_max.shape != (m_max,):\n",
    "        raise ValueError(f'prior_diag_max must have shape ({m_max},), got {prior_diag_max.shape}')\n",
    "    if a_mode == 'nearest_neighbor':\n",
    "        omegas = make_omegas_power(d_max, beta=alpha, c=2.0 ** (-gamma), offset=offset)\n",
    "        A_true_max = make_Astar_nn(d_max, omegas)\n",
    "        a_true_max = params_from_skew(A_true_max)\n",
    "    elif a_mode == 'prior':\n",
    "        z_prior = rng.standard_normal(m_max)\n",
    "        a_true_max = z_prior * np.sqrt(prior_diag_max)\n",
    "        A_true_max = make_Astar_from_atrue(d_max, a_true_max)\n",
    "    else:\n",
    "        raise ValueError(\"a_mode must be 'nearest_neighbor' or 'prior'\")\n",
    "    g_max = np.zeros(d_max, dtype=float)\n",
    "    g_max[0] = 1.0\n",
    "    theta_true_max = solve_theta(d_max, a_true_max, g_max, kappa)\n",
    "    noise_max = rng.standard_normal(d_max)\n",
    "    z_init = rng.standard_normal(m_max)\n",
    "    a_init_max = z_init * np.sqrt(prior_diag_max)\n",
    "    return {\n",
    "        'd_max': d_max,\n",
    "        'm_max': m_max,\n",
    "        'kappa': kappa,\n",
    "        'sigma': sigma,\n",
    "        'alpha': alpha,\n",
    "        'gamma': gamma,\n",
    "        'tau2': tau2,\n",
    "        'offset': offset,\n",
    "        'a_mode': a_mode,\n",
    "        'param_iju': np.triu_indices(d_max, k=1),\n",
    "        'param_indices_cache': {},\n",
    "        'prior_diag': prior_diag_max,\n",
    "        'a_true': a_true_max,\n",
    "        'A_true': A_true_max,\n",
    "        'g': g_max,\n",
    "        'theta_true': theta_true_max,\n",
    "        'noise': noise_max,\n",
    "        'a_init': a_init_max,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advection_diffusion_data_shared(dim, obs_indices, shared_draws):\n",
    "    a_mode_local = shared_draws['a_mode']\n",
    "    param_idx = get_param_indices_for_dim(dim, shared_draws)\n",
    "    prior_diag = shared_draws['prior_diag'][param_idx]\n",
    "    g = shared_draws['g'][:dim]\n",
    "    if a_mode_local == 'nearest_neighbor':\n",
    "        omegas = make_omegas_power(\n",
    "            dim,\n",
    "            beta=shared_draws['alpha'],\n",
    "            c=2.0 ** (-shared_draws['gamma']),\n",
    "            offset=shared_draws['offset'],\n",
    "        )\n",
    "        A_true = make_Astar_nn(dim, omegas)\n",
    "        a_true = params_from_skew(A_true)\n",
    "        theta_true = solve_theta(dim, a_true, g, shared_draws['kappa'])\n",
    "    elif a_mode_local == 'prior':\n",
    "        a_true = shared_draws['a_true'][param_idx]\n",
    "        A_true = make_Astar_from_atrue(dim, a_true)\n",
    "        theta_true = shared_draws['theta_true'][:dim]\n",
    "    else:\n",
    "        raise ValueError(\"a_mode must be 'nearest_neighbor' or 'prior'\")\n",
    "    noise = shared_draws['noise'][:dim]\n",
    "    y = theta_true[obs_indices] + shared_draws['sigma'] * noise[obs_indices]\n",
    "    a_init = shared_draws['a_init'][param_idx]\n",
    "    return {\n",
    "        'dim': dim,\n",
    "        'kappa': shared_draws['kappa'],\n",
    "        'alpha': shared_draws['alpha'],\n",
    "        'gamma': shared_draws['gamma'],\n",
    "        'tau2': shared_draws['tau2'],\n",
    "        'sigma': shared_draws['sigma'],\n",
    "        'obs_indices': obs_indices,\n",
    "        'prior_diag': prior_diag,\n",
    "        'a_true': a_true,\n",
    "        'A_true': A_true,\n",
    "        'g': g,\n",
    "        'theta_true': theta_true,\n",
    "        'y': y,\n",
    "        'a_init': a_init,\n",
    "    }\n",
    "\n",
    "def get_dataset_for_dim(d, seed=0):\n",
    "    if d in datasets_by_dim:\n",
    "        return datasets_by_dim[d]\n",
    "    obs_indices = get_obs_indices(d, obs_highest_freq, obs_bandwidth)\n",
    "    data = generate_advection_diffusion_data_shared(d, obs_indices, shared_draws)\n",
    "    data['obs_indices'] = obs_indices\n",
    "    datasets_by_dim[d] = data\n",
    "    return data\n",
    "\n",
    "def build_problem_for_dim(d, seed=0):\n",
    "    data = get_dataset_for_dim(d, seed=seed)\n",
    "    obs_indices = data['obs_indices']\n",
    "    problem = AdvectionDiffusionToy(\n",
    "        dim=d,\n",
    "        kappa=kappa,\n",
    "        sigma=sigma,\n",
    "        y=data['y'],\n",
    "        obs_indices=obs_indices,\n",
    "        g=data['g'],\n",
    "        prior_diag=data['prior_diag'],\n",
    "    )\n",
    "    return problem, data['a_init'], obs_indices, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_draws = build_shared_draws(\n",
    "    d_max=d_max,\n",
    "    kappa=kappa,\n",
    "    sigma=sigma,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    tau2=tau2,\n",
    "    offset=1.0,\n",
    "    a_mode='prior' if use_prior_A else a_mode,\n",
    "    seed=shared_draws_seed,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb29ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check: A_true and observations for each d (including d_max).\n",
    "plot_dims = [10, 20, 40]\n",
    "n_cols = len(plot_dims)\n",
    "fig, axes = plt.subplots(2, n_cols, figsize=(12, 6))\n",
    "axes = np.atleast_2d(axes)\n",
    "\n",
    "row_label_size = 14\n",
    "title_size = 13\n",
    "tick_size = 11\n",
    "axis_label_size = 12\n",
    "cbar_tick_size = 11\n",
    "\n",
    "last_im = None\n",
    "for col_idx, d_cur in enumerate(plot_dims):\n",
    "    data = get_dataset_for_dim(d_cur, seed=seed_data)\n",
    "\n",
    "    ax_A = axes[0, col_idx]\n",
    "    ax_obs = axes[1, col_idx]\n",
    "\n",
    "    last_im = ax_A.imshow(data['A_true'], cmap='coolwarm', aspect='auto')\n",
    "    if col_idx == 0:\n",
    "        ax_A.set_ylabel('i', fontsize=axis_label_size)\n",
    "    ax_A.set_xlabel('j', fontsize=axis_label_size)\n",
    "    ax_A.tick_params(axis='both', labelsize=tick_size)\n",
    "\n",
    "    theta_true = data['theta_true']\n",
    "    obs_indices = data['obs_indices']\n",
    "    ax_obs.plot(\n",
    "        np.arange(d_cur),\n",
    "        theta_true,\n",
    "        color='tab:blue',\n",
    "        label=r\"$\\mathbf{\\Theta}(\\mathbf{A})$\",\n",
    "    )\n",
    "    ax_obs.scatter(obs_indices, data['y'], color='tab:orange', s=20, label=r\"$y$\")\n",
    "    if col_idx == 0:\n",
    "        ax_obs.set_ylabel('Value', fontsize=axis_label_size)\n",
    "    ax_obs.set_xlabel('i', fontsize=axis_label_size)\n",
    "    ax_obs.grid(alpha=0.2)\n",
    "    ax_obs.legend(loc='best', fontsize=tick_size)\n",
    "    ax_obs.tick_params(axis='both', labelsize=tick_size)\n",
    "\n",
    "    ax_A.set_title(rf\"$d={d_cur}$\", fontsize=title_size)\n",
    "\n",
    "if last_im is not None:\n",
    "    cbar = fig.colorbar(last_im, ax=axes[0, -1], fraction=0.05, pad=0.02)\n",
    "    cbar.ax.tick_params(labelsize=cbar_tick_size)\n",
    "\n",
    "fig.text(0.01, 0.73, r\"$\\mathbf{A}$\", rotation=0, va='center', ha='left', fontsize=row_label_size)\n",
    "fig.text(\n",
    "    0.01,\n",
    "    0.27,\n",
    "    \"Obs.\",\n",
    "    rotation=0,\n",
    "    va='center',\n",
    "    ha='left',\n",
    "    fontsize=row_label_size,\n",
    " )\n",
    "fig.tight_layout(rect=[0.05, 0.02, 0.95, 0.98])\n",
    "\n",
    "fig_path = reports_dir / \"visual_check_A_theta_y.png\"\n",
    "fig.savefig(fig_path, dpi=600, bbox_inches=\"tight\")\n",
    "print(f\"Saved {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86a458",
   "metadata": {},
   "source": [
    "## Tune MH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fe43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automated MH tuning (d=20).\n",
    "# tune_d = 20\n",
    "# tune_iters = 30000\n",
    "# target_accept = 0.234\n",
    "# sigma_center = 0.05\n",
    "# grid_factors = [2.1, 2.2, 2.3, 2.4, 2.5] # [0.2, 0.35, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0]\n",
    "# sigma_grid = [sigma_center * f for f in grid_factors]\n",
    "\n",
    "# problem_cur, x0_cur, obs_indices_cur, _ = build_problem_for_dim(tune_d, seed=seed_data)\n",
    "# print(f'Tuning MH at d={tune_d} with obs_indices={obs_indices_cur.tolist()}')\n",
    "\n",
    "# results = []\n",
    "# for proposal_std in sigma_grid:\n",
    "#     rng = np.random.default_rng(seed_mcmc)\n",
    "#     chain_mh, acc = mh_chain(\n",
    "#         x0_cur, problem_cur, rng, tune_iters, proposal_std=proposal_std, proposal_cov=mh_proposal_cov\n",
    "#     )\n",
    "#     results.append((proposal_std, acc))\n",
    "#     print(f'std={proposal_std:.8g} | acc={acc:.4f}')\n",
    "\n",
    "# best_std, best_acc = min(results, key=lambda item: abs(item[1] - target_accept))\n",
    "# print('\\nBest proposal std (closest to target acceptance):')\n",
    "# print(f'std={best_std:.8g} | acc={best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e23c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Profiling: break down MH step costs by component.\n",
    "# # Adjust dims_to_profile and n_reps for quick checks.\n",
    "# import time\n",
    "# import math\n",
    "\n",
    "# dims_to_profile = [10, 20, 30, 40]\n",
    "# n_reps = 200\n",
    "# proposal_cov_to_profile = mh_proposal_cov  # 'prior' or 'isotropic'\n",
    "\n",
    "# def _timed(fn, n_calls=1):\n",
    "#     t0 = time.perf_counter()\n",
    "#     out = None\n",
    "#     for _ in range(n_calls):\n",
    "#         out = fn()\n",
    "#     t1 = time.perf_counter()\n",
    "#     return (t1 - t0) / max(1, n_calls), out\n",
    "\n",
    "# def _profile_dim(d_cur):\n",
    "#     problem_cur, x0_cur, _, _ = build_problem_for_dim(d_cur, seed=seed_data)\n",
    "#     rng = np.random.default_rng(seed_mcmc)\n",
    "#     x = x0_cur.copy()\n",
    "\n",
    "#     # Proposal generation cost.\n",
    "#     def _proposal_only():\n",
    "#         if proposal_cov_to_profile == \"prior\":\n",
    "#             z = rng.standard_normal(problem_cur.dim)\n",
    "#             return x + mh_proposal_std_chosen * (problem_cur.L @ z)\n",
    "#         return x + mh_proposal_std_chosen * rng.standard_normal(problem_cur.dim)\n",
    "\n",
    "#     # Log-density components.\n",
    "#     def _log_prior():\n",
    "#         return problem_cur.log_prior(x)\n",
    "\n",
    "#     def _log_likelihood():\n",
    "#         return problem_cur.log_likelihood(x)\n",
    "\n",
    "#     def _log_posterior():\n",
    "#         return problem_cur.log_posterior(x)\n",
    "\n",
    "#     # Theta solve only (often dominant).\n",
    "#     def _theta_solve():\n",
    "#         _ = problem_cur.theta_from_params(x)\n",
    "#         return None\n",
    "\n",
    "#     proposal_t, _ = _timed(_proposal_only, n_reps)\n",
    "#     prior_t, _ = _timed(_log_prior, n_reps)\n",
    "#     like_t, _ = _timed(_log_likelihood, n_reps)\n",
    "#     post_t, _ = _timed(_log_posterior, n_reps)\n",
    "#     theta_t, _ = _timed(_theta_solve, n_reps)\n",
    "\n",
    "#     # Rough per-MH-step estimate: proposal + 2 * log_posterior evaluations.\n",
    "#     approx_step = proposal_t + 2.0 * post_t\n",
    "\n",
    "#     return {\n",
    "#         \"d\": d_cur,\n",
    "#         \"proposal_ms\": 1e3 * proposal_t,\n",
    "#         \"log_prior_ms\": 1e3 * prior_t,\n",
    "#         \"log_like_ms\": 1e3 * like_t,\n",
    "#         \"log_post_ms\": 1e3 * post_t,\n",
    "#         \"theta_solve_ms\": 1e3 * theta_t,\n",
    "#         \"approx_step_ms\": 1e3 * approx_step,\n",
    "#     }\n",
    "\n",
    "# print(\"MH timing breakdown (per-call milliseconds):\")\n",
    "# for d_cur in dims_to_profile:\n",
    "#     stats = _profile_dim(d_cur)\n",
    "#     print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073caf06",
   "metadata": {},
   "source": [
    "## Run Sweep and Save Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775582c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import repeat\n",
    "\n",
    "def _run_dim_task(d_idx, d_cur, config):\n",
    "    seed_mcmc = config['seed_mcmc']\n",
    "    seed_data = config['seed_data']\n",
    "    n_iters_dim = config['n_iters_dim']\n",
    "    burn_in_cur = config['burn_in']\n",
    "    thin = config['thin']\n",
    "    M_list = config['M_list']\n",
    "    run_mess = config['run_mess']\n",
    "    run_mh = config['run_mh']\n",
    "    output_dir = Path(config['output_dir'])\n",
    "    mh_proposal_stds_scaled = config['mh_proposal_stds_scaled']\n",
    "    mh_proposal_cov = config['mh_proposal_cov']\n",
    "    recompute_corrupt_chains = config['recompute_corrupt_chains']\n",
    "\n",
    "    problem_cur, x0_cur, obs_indices_cur, _ = build_problem_for_dim(d_cur, seed=seed_data)\n",
    "    print(f'--- d={d_cur} | burn_in={burn_in_cur} | obs_indices={obs_indices_cur.tolist()}')\n",
    "\n",
    "    # MESS sweep\n",
    "    if run_mess:\n",
    "        for M in M_list:\n",
    "            out_path = chain_path(output_dir, d_cur, 'mess', M=M)\n",
    "            if out_path.exists() and is_chain_readable(out_path):\n",
    "                print(f'Skip existing: {out_path.name}')\n",
    "                continue\n",
    "            if out_path.exists() and not is_chain_readable(out_path):\n",
    "                if recompute_corrupt_chains:\n",
    "                    print(f'Recomputing corrupt chain: {out_path.name}')\n",
    "                else:\n",
    "                    print(f'Corrupt chain found, skipping: {out_path.name}')\n",
    "                    continue\n",
    "            print(f'\\nStart MESS chain: d={d_cur}, M={M}, n_iters={n_iters_dim}, burn_in={burn_in_cur}, thin={thin}')\n",
    "            rng = np.random.default_rng(seed_mcmc)\n",
    "            chain = np.zeros((n_iters_dim + 1, x0_cur.shape[0]))\n",
    "            chain[0] = x0_cur.copy()\n",
    "            x = x0_cur.copy()\n",
    "            t0 = time.perf_counter()\n",
    "            for t in range(n_iters_dim):\n",
    "                x, _, _ = mess_step(x, problem_cur, rng, M=M, use_lp=False)\n",
    "                chain[t + 1] = x\n",
    "            t1 = time.perf_counter()\n",
    "            post = chain[::thin]\n",
    "            metadata = {\n",
    "                'alg': 'mess',\n",
    "                'M': M,\n",
    "                'd': d_cur,\n",
    "                'n_iters': n_iters_dim,\n",
    "                'burn_in': burn_in_cur,\n",
    "                'thin': thin,\n",
    "                'seed_mcmc': seed_mcmc,\n",
    "                'seed_data': seed_data,\n",
    "                'runtime_sec': t1 - t0,\n",
    "            }\n",
    "            save_chain(out_path, post, metadata)\n",
    "            print(f'Saved {out_path.name}')\n",
    "\n",
    "    # MH sweep\n",
    "    if run_mh:\n",
    "        mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "        out_path = chain_path(\n",
    "            output_dir,\n",
    "            d_cur,\n",
    "            'mh',\n",
    "            proposal_std=mh_std,\n",
    "            proposal_cov=mh_proposal_cov,\n",
    "        )\n",
    "        if out_path.exists() and is_chain_readable(out_path):\n",
    "            print(f'Skip existing: {out_path.name}')\n",
    "        else:\n",
    "            if out_path.exists() and not is_chain_readable(out_path):\n",
    "                if recompute_corrupt_chains:\n",
    "                    print(f'Recomputing corrupt chain: {out_path.name}')\n",
    "                else:\n",
    "                    print(f'Corrupt chain found, skipping: {out_path.name}')\n",
    "                    return\n",
    "            print(\n",
    "                f'\\nStart MH chain: d={d_cur}, n_iters={n_iters_dim}, burn_in={burn_in_cur}, thin={thin}, proposal_std={mh_std}, proposal_cov={mh_proposal_cov}'\n",
    "            )\n",
    "            rng = np.random.default_rng(seed_mcmc)\n",
    "            t0 = time.perf_counter()\n",
    "            chain_mh, acc = mh_chain(\n",
    "                x0_cur,\n",
    "                problem_cur,\n",
    "                rng,\n",
    "                n_iters_dim,\n",
    "                proposal_std=mh_std,\n",
    "                proposal_cov=mh_proposal_cov,\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            post = chain_mh[::thin]\n",
    "            metadata = {\n",
    "                'alg': 'mh',\n",
    "                'd': d_cur,\n",
    "                'n_iters': n_iters_dim,\n",
    "                'burn_in': burn_in_cur,\n",
    "                'thin': thin,\n",
    "                'seed_mcmc': seed_mcmc,\n",
    "                'seed_data': seed_data,\n",
    "                'proposal_std': mh_std,\n",
    "                'proposal_cov': mh_proposal_cov,\n",
    "                'acceptance': acc,\n",
    "                'runtime_sec': t1 - t0,\n",
    "            }\n",
    "            save_chain(out_path, post, metadata)\n",
    "            print(f'Saved {out_path.name}')\n",
    "\n",
    "if not run_mess and not run_mh:\n",
    "    print('Set run_mess and/or run_mh to True to generate chains.')\n",
    "else:\n",
    "    # High-level run context.\n",
    "    print('Starting sweep with config:')\n",
    "    print({\n",
    "        'seed_mcmc': seed_mcmc,\n",
    "        'seed_data': seed_data,\n",
    "        'n_iters_dim': n_iters,\n",
    "        'burn_in': burn_in,\n",
    "        'thin': thin,\n",
    "        'max_lag': max_lag,\n",
    "        'd_list': d_list,\n",
    "        'M_list': M_list,\n",
    "        'output_dir': str(output_dir),\n",
    "        'run_mess': run_mess,\n",
    "        'run_mh': run_mh,\n",
    "        'mh_proposal_cov': mh_proposal_cov,\n",
    "        'use_parallel': use_parallel,\n",
    "        'max_workers': max_workers,\n",
    "        'recompute_corrupt_chains': recompute_corrupt_chains,\n",
    "    })\n",
    "\n",
    "    config = {\n",
    "        'seed_mcmc': seed_mcmc,\n",
    "        'seed_data': seed_data,\n",
    "        'n_iters_dim': n_iters,\n",
    "        'burn_in': burn_in,\n",
    "        'thin': thin,\n",
    "        'M_list': M_list,\n",
    "        'run_mess': run_mess,\n",
    "        'run_mh': run_mh,\n",
    "        'output_dir': str(output_dir),\n",
    "        'mh_proposal_stds_scaled': mh_proposal_stds_scaled,\n",
    "        'mh_proposal_cov': mh_proposal_cov,\n",
    "        'recompute_corrupt_chains': recompute_corrupt_chains,\n",
    "    }\n",
    "\n",
    "    tasks = list(enumerate(d_list))\n",
    "    if use_parallel and max_workers > 1:\n",
    "        print(f'Parallel sweep by dimension with max_workers={max_workers}')\n",
    "        try:\n",
    "            mp_context = mp.get_context('fork')\n",
    "        except ValueError:\n",
    "            mp_context = None\n",
    "        if mp_context is None:\n",
    "            print('Fork context unavailable; falling back to serial execution.')\n",
    "            for d_idx, d_cur in tasks:\n",
    "                _run_dim_task(d_idx, d_cur, config)\n",
    "        else:\n",
    "            with ProcessPoolExecutor(max_workers=max_workers, mp_context=mp_context) as executor:\n",
    "                list(executor.map(_run_dim_task, [t[0] for t in tasks], [t[1] for t in tasks], repeat(config)))\n",
    "    else:\n",
    "        print('Serial sweep by dimension')\n",
    "        for d_idx, d_cur in tasks:\n",
    "            _run_dim_task(d_idx, d_cur, config)\n",
    "\n",
    "print(f\"Sweep completed. Chains saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ce586",
   "metadata": {},
   "source": [
    "## Traceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality traceplots: components 1,2,9; d=10,25,30; MESS M=1/M=50 and MH.\n",
    "trace_iters = 10000\n",
    "plot_dims = d_list\n",
    "plot_components = [0, 1, 2, 9]\n",
    "plot_Ms = [1, 50]\n",
    "save_figs = True\n",
    "trace_dir = reports_dir / \"traceplots_pub\"\n",
    "trace_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120\n",
    "    ,\"savefig.dpi\": 600\n",
    "    ,\"font.size\": 14\n",
    "    ,\"axes.titlesize\": 15\n",
    "    ,\"axes.labelsize\": 14\n",
    "    ,\"axes.linewidth\": 0.9\n",
    "    ,\"xtick.labelsize\": 12\n",
    "    ,\"ytick.labelsize\": 12\n",
    "    ,\"legend.fontsize\": 12\n",
    "    ,\"lines.linewidth\": 0.9\n",
    "})\n",
    "\n",
    "M_color_values = [1, 10, 20, 50, 100, 200, 300]\n",
    "M_colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(M_color_values)))\n",
    "M_color_map = {M: color for M, color in zip(M_color_values, M_colors)}\n",
    "\n",
    "algorithms = [\n",
    "    (\"mess\", \"MESS (M=1)\", plot_Ms[0], M_color_map.get(plot_Ms[0], \"#1b9e77\")),\n",
    "    (\"mess\", \"MESS (M=50)\", plot_Ms[1], M_color_map.get(plot_Ms[1], \"#d95f02\")),\n",
    "    (\"mh\", \"MH\", None, \"#7570b3\"),\n",
    "]\n",
    "\n",
    "def _load_trace_chain(d_cur, alg_key, M=None):\n",
    "    if alg_key == \"mh\":\n",
    "        d_idx = d_list.index(d_cur)\n",
    "        mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "        return load_chain(\n",
    "            output_dir,\n",
    "            d_cur,\n",
    "            \"mh\",\n",
    "            proposal_std=mh_std,\n",
    "            proposal_cov=mh_proposal_cov,\n",
    "        )\n",
    "    return load_chain(output_dir, d_cur, \"mess\", M=M)\n",
    "\n",
    "def _component_label(dim_value, comp_idx):\n",
    "    count = 0\n",
    "    for i in range(dim_value):\n",
    "        for j in range(i + 1, dim_value):\n",
    "            if count == comp_idx:\n",
    "                return f\"a_{{{i}{j}}}\"\n",
    "            count += 1\n",
    "    return f\"param_{comp_idx}\"\n",
    "\n",
    "def _warn_if_flat(chain, comp_idx, label):\n",
    "    if chain is None or chain.size == 0 or comp_idx >= chain.shape[1]:\n",
    "        return\n",
    "    series = chain[:trace_iters, comp_idx]\n",
    "    series_std = float(np.std(series))\n",
    "    if series_std == 0.0:\n",
    "        print(f\"Warning: flat trace for {label} comp={comp_idx} (std=0).\")\n",
    "    elif series_std < 1e-12:\n",
    "        print(f\"Warning: nearly flat trace for {label} comp={comp_idx} (std={series_std:.2e}).\")\n",
    "\n",
    "for comp in plot_components:\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(plot_dims),\n",
    "        ncols=len(algorithms),\n",
    "        figsize=(10.5, 6.5),\n",
    "        sharex=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    axes = np.atleast_2d(axes)\n",
    "    comp_label = _component_label(max(plot_dims), comp)\n",
    "    fig.suptitle(f\"Traceplots for ${comp_label}$ (first {trace_iters} iterations)\")\n",
    "\n",
    "    # Preload chains and compute per-dimension y-limits.\n",
    "    chain_cache = {}\n",
    "    y_limits = {}\n",
    "    for d_cur in plot_dims:\n",
    "        for alg_key, alg_label, M, _ in algorithms:\n",
    "            chain = _load_trace_chain(d_cur, alg_key, M=M)\n",
    "            chain_cache[(d_cur, alg_key, M)] = chain\n",
    "            if chain is None or chain.size == 0:\n",
    "                continue\n",
    "            series = chain[:trace_iters, comp]\n",
    "            if d_cur not in y_limits:\n",
    "                y_limits[d_cur] = [series.min(), series.max()]\n",
    "            else:\n",
    "                y_limits[d_cur][0] = min(y_limits[d_cur][0], series.min())\n",
    "                y_limits[d_cur][1] = max(y_limits[d_cur][1], series.max())\n",
    "\n",
    "    for row_idx, d_cur in enumerate(plot_dims):\n",
    "        for col_idx, (alg_key, alg_label, M, color) in enumerate(algorithms):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            chain = chain_cache[(d_cur, alg_key, M)]\n",
    "            if chain is None or chain.size == 0:\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            series = chain[:trace_iters, comp]\n",
    "            ax.plot(series, color=color, alpha=0.85)\n",
    "            ax.set_ylim(y_limits[d_cur])\n",
    "            if row_idx == 0:\n",
    "                ax.set_title(alg_label)\n",
    "            if col_idx == 0:\n",
    "                ax.set_ylabel(f\"d={d_cur}\")\n",
    "            if row_idx == len(plot_dims) - 1:\n",
    "                ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "            _warn_if_flat(chain, comp, f\"{alg_label} (d={d_cur})\")\n",
    "\n",
    "    if save_figs:\n",
    "        fig_path = trace_dir / f\"traceplots_comp{comp}.png\"\n",
    "        fig.savefig(fig_path, bbox_inches=\"tight\")\n",
    "        print(f\"Saved {fig_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace/hist panel with stacked traces on left, histograms on right.\n",
    "trace_iters = 30000\n",
    "hist_bins = 30\n",
    "panel_comps = [0, 1]\n",
    "hist_d = d_list[0]\n",
    "font_size = 18\n",
    "mess_plot_Ms = [1, 10, 50, 100]\n",
    "save_figs = True\n",
    "panel_dir = reports_dir / \"trace_hist_panels\"\n",
    "panel_dir.mkdir(parents=True, exist_ok=True)\n",
    "trace_xticks = [0, 5000, 10000, 15000, 20000]\n",
    "\n",
    "def get_component_labels(dim_value, comps):\n",
    "    labels = []\n",
    "    for k in comps:\n",
    "        count = 0\n",
    "        found = False\n",
    "        for i in range(dim_value):\n",
    "            for j in range(i + 1, dim_value):\n",
    "                if count == k:\n",
    "                    labels.append(f\"a_{{{i}{j}}}\")\n",
    "                    found = True\n",
    "                    break\n",
    "                count += 1\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            labels.append(f\"param_{k}\")\n",
    "    return labels\n",
    "\n",
    "def _load_chain_for_plot(d_cur, alg, M=None):\n",
    "    if alg == 'mh':\n",
    "        d_idx = d_list.index(d_cur)\n",
    "        mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "        return (\n",
    "            load_chain(\n",
    "                output_dir,\n",
    "                d_cur,\n",
    "                'mh',\n",
    "                proposal_std=mh_std,\n",
    "                proposal_cov=mh_proposal_cov,\n",
    "            ),\n",
    "            f'MH d={d_cur}',\n",
    "        )\n",
    "    return load_chain(output_dir, d_cur, 'mess', M=M), f'MESS d={d_cur} M={M}'\n",
    "\n",
    "def plot_trace_hist_panel(d_cur, comp):\n",
    "    mess_colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(M_list)))\n",
    "    mess_color_map = {M: color for M, color in zip(M_list, mess_colors)}\n",
    "    trace_colors = {\n",
    "        'mh': 'black',\n",
    "        **{f'mess{M}': mess_color_map.get(M, 'gray') for M in mess_plot_Ms},\n",
    "    }\n",
    "    algorithms = [\n",
    "        ('mess100', 'MESS (M=100)', trace_colors['mess100'], 100),\n",
    "        ('mess50', 'MESS (M=50)', trace_colors['mess50'], 50),\n",
    "        ('mess10', 'MESS (M=10)', trace_colors['mess10'], 10),\n",
    "        ('mess1', 'MESS (M=1)', trace_colors['mess1'], 1),\n",
    "        ('mh', 'MH', trace_colors['mh'], None),\n",
    "    ]\n",
    "    hist_algorithms = [\n",
    "        ('mh', 'MH', trace_colors['mh'], None),\n",
    "        ('mess1', 'MESS (M=1)', trace_colors['mess1'], 1),\n",
    "        ('mess10', 'MESS (M=10)', trace_colors['mess10'], 10),\n",
    "        ('mess50', 'MESS (M=50)', trace_colors['mess50'], 50),\n",
    "        ('mess100', 'MESS (M=100)', trace_colors['mess100'], 100),\n",
    "    ]\n",
    "    chains_dict = {}\n",
    "    labels_dict = {}\n",
    "    for key, label, _, M in algorithms:\n",
    "        chain, _ = _load_chain_for_plot(d_cur, 'mh' if key == 'mh' else 'mess', M=M)\n",
    "        if chain is None or chain.size == 0:\n",
    "            print(f'Missing chain for {label} at d={d_cur}')\n",
    "            return\n",
    "        chains_dict[key] = chain[burn_in::thin]\n",
    "        labels_dict[key] = label\n",
    "\n",
    "    data_hist = get_dataset_for_dim(d_cur, seed=seed_data)\n",
    "    true_val = float(data_hist['a_true'][comp])\n",
    "    label_map = dict(zip(panel_comps, get_component_labels(d_cur, panel_comps)))\n",
    "\n",
    "    n_algs = len(algorithms)\n",
    "    fig = plt.figure(figsize=(13, 5.0))\n",
    "    gs = fig.add_gridspec(n_algs, 2, width_ratios=[2.0, 1.2], wspace=0.25, hspace=0.35)\n",
    "\n",
    "    all_data = [chains_dict[key][:trace_iters, comp] for key, _, _, _ in algorithms]\n",
    "    data_min = np.min([np.min(d) for d in all_data])\n",
    "    data_max = np.max([np.max(d) for d in all_data])\n",
    "    data_range = data_max - data_min if data_max != data_min else 1.0\n",
    "\n",
    "    # Stacked traces\n",
    "    for alg_idx, (alg_key, alg_label, color, _) in enumerate(algorithms):\n",
    "        ax_trace = fig.add_subplot(gs[alg_idx, 0])\n",
    "        series = chains_dict[alg_key][:trace_iters, comp]\n",
    "        ax_trace.plot(series, color=color, linewidth=0.5, label=alg_label)\n",
    "        ax_trace.set_ylim([data_min - 0.05 * data_range, data_max + 0.05 * data_range])\n",
    "        ax_trace.set_xlim([0, trace_iters])\n",
    "        ax_trace.set_xticks(trace_xticks)\n",
    "        ax_trace.grid(alpha=0.2)\n",
    "        ax_trace.tick_params(labelsize=font_size - 2)\n",
    "        ax_trace.spines['top'].set_visible(False)\n",
    "        ax_trace.spines['right'].set_visible(False)\n",
    "        if alg_idx < n_algs - 1:\n",
    "            ax_trace.set_xticklabels([])\n",
    "        else:\n",
    "            ax_trace.set_xlabel('Iteration', fontsize=font_size)\n",
    "\n",
    "    # Histogram panel\n",
    "    ax_hist = fig.add_subplot(gs[:, 1])\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    for alg_key, alg_label, color, _ in hist_algorithms:\n",
    "        _, _, patches = ax_hist.hist(\n",
    "            chains_dict[alg_key][:, comp],\n",
    "            bins=hist_bins,\n",
    "            density=True,\n",
    "            alpha=0.35,\n",
    "            color=color,\n",
    "            label=alg_label,\n",
    "        )\n",
    "        if patches:\n",
    "            legend_handles.append(patches[0])\n",
    "            legend_labels.append(alg_label)\n",
    "    true_line = ax_hist.axvline(\n",
    "        true_val,\n",
    "        color='black',\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        label='True value',\n",
    "    )\n",
    "    ax_hist.set_xlabel('Value', fontsize=font_size)\n",
    "    ax_hist.set_ylabel('Density', fontsize=font_size)\n",
    "    ax_hist.grid(alpha=0.2)\n",
    "    ax_hist.tick_params(labelsize=font_size - 2)\n",
    "    comp_label = label_map.get(comp, f'param {comp}')\n",
    "\n",
    "    legend_order = ['MH', 'MESS (M=1)', 'MESS (M=10)', 'MESS (M=50)', 'MESS (M=100)']\n",
    "    legend_map = dict(zip(legend_labels, legend_handles))\n",
    "    ordered_labels = [label for label in legend_order if label in legend_map]\n",
    "    ordered_handles = [legend_map[label] for label in ordered_labels]\n",
    "    if len(ordered_labels) != len(legend_labels):\n",
    "        ordered_handles = legend_handles\n",
    "        ordered_labels = legend_labels\n",
    "\n",
    "    fig.legend(\n",
    "        ordered_handles,\n",
    "        ordered_labels,\n",
    "        loc='upper center',\n",
    "        ncol=len(ordered_labels),\n",
    "        bbox_to_anchor=(0.5, 1.04),\n",
    "        fontsize=font_size - 2,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax_hist.legend(\n",
    "        handles=[true_line],\n",
    "        labels=['True value'],\n",
    "        loc='lower center',\n",
    "        bbox_to_anchor=(0.5, 0.98),\n",
    "        fontsize=font_size - 2,\n",
    "        frameon=False,\n",
    "        borderaxespad=0.0,\n",
    "    )\n",
    "    plt.tight_layout(rect=[0.0, 0.0, 1.0, 0.86])\n",
    "    if save_figs:\n",
    "        fig_path = panel_dir / f\"trace_hist_d{d_cur}_comp{comp}.png\"\n",
    "        fig.savefig(fig_path, dpi=600, bbox_inches=\"tight\")\n",
    "        print(f\"Saved {fig_path}\")\n",
    "    plt.show()\n",
    "\n",
    "for comp in panel_comps:\n",
    "    plot_trace_hist_panel(hist_d, comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e256773",
   "metadata": {},
   "source": [
    "## Load Chains and Compute ESS/MSJD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20925ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_by_dim(\n",
    "    burnin=5000,\n",
    "    max_lag=1500,\n",
    "    components=None,\n",
    "    metrics_path=None,\n",
    "    force_recompute=False,\n",
    "    save_every_update=True,\n",
    " ):\n",
    "    def _unwrap_loaded(value):\n",
    "        if isinstance(value, np.ndarray) and value.dtype == object and value.size == 1:\n",
    "            return value.item()\n",
    "        if isinstance(value, np.ndarray):\n",
    "            return value.tolist()\n",
    "        return value\n",
    "\n",
    "    def _ensure_list(values, length, fill_value=np.nan):\n",
    "        values = list(values) if values is not None else []\n",
    "        if len(values) < length:\n",
    "            values = values + [fill_value] * (length - len(values))\n",
    "        if len(values) > length:\n",
    "            values = values[:length]\n",
    "        return values\n",
    "\n",
    "    def _ensure_by_M(metrics_dict, fill_value=np.nan):\n",
    "        metrics_dict = metrics_dict or {}\n",
    "        for M in M_list:\n",
    "            metrics_dict[M] = _ensure_list(metrics_dict.get(M, []), len(d_list), fill_value=fill_value)\n",
    "        return metrics_dict\n",
    "\n",
    "    def _ensure_by_M_components(metrics_dict, components_count):\n",
    "        metrics_dict = metrics_dict or {}\n",
    "        fill_value = [np.nan] * components_count\n",
    "        for M in M_list:\n",
    "            metrics_dict[M] = _ensure_list(\n",
    "                metrics_dict.get(M, []),\n",
    "                len(d_list),\n",
    "                fill_value=fill_value,\n",
    "            )\n",
    "        return metrics_dict\n",
    "\n",
    "    def _is_missing(val):\n",
    "        if val is None:\n",
    "            return True\n",
    "        try:\n",
    "            return bool(np.isnan(val))\n",
    "        except TypeError:\n",
    "            return False\n",
    "\n",
    "    def _apply_burnin(chain):\n",
    "        if chain is None:\n",
    "            return None\n",
    "        if burnin is None or burnin <= 0:\n",
    "            return chain\n",
    "        if chain.shape[0] <= burnin:\n",
    "            return chain[:0]\n",
    "        return chain[burnin:]\n",
    "\n",
    "    def _normalize_components(components_in, dim):\n",
    "        if components_in is None:\n",
    "            return list(range(dim))\n",
    "        return [int(c) for c in components_in]\n",
    "\n",
    "    def _split_components(components_idx, dim):\n",
    "        valid_indices = []\n",
    "        valid_positions = []\n",
    "        for pos, idx in enumerate(components_idx):\n",
    "            if 0 <= idx < dim:\n",
    "                valid_indices.append(idx)\n",
    "                valid_positions.append(pos)\n",
    "        return valid_indices, valid_positions\n",
    "\n",
    "    def _compute_from_chain(chain, components_idx):\n",
    "        components_count = len(components_idx)\n",
    "        selected_ess = [np.nan] * components_count\n",
    "        selected_msjd = [np.nan] * components_count\n",
    "        if chain is None or chain.size == 0:\n",
    "            return selected_ess, selected_msjd\n",
    "        dim = chain.shape[1]\n",
    "        valid_indices, valid_positions = _split_components(components_idx, dim)\n",
    "        if not valid_indices:\n",
    "            return selected_ess, selected_msjd\n",
    "        chain_sel = chain[:, valid_indices]\n",
    "        ess_vals = compute_ess_per_param(chain_sel, max_lag=max_lag)\n",
    "        msjd_vals = compute_msjd_per_param(chain_sel)\n",
    "        for pos, val in zip(valid_positions, ess_vals):\n",
    "            selected_ess[pos] = float(val)\n",
    "        for pos, val in zip(valid_positions, msjd_vals):\n",
    "            selected_msjd[pos] = float(val)\n",
    "        return selected_ess, selected_msjd\n",
    "\n",
    "    def _mean_selected(selected):\n",
    "        arr = np.array(selected, dtype=float)\n",
    "        if arr.size == 0:\n",
    "            return np.nan\n",
    "        if np.all(np.isnan(arr)):\n",
    "            return np.nan\n",
    "        return float(np.nanmean(arr))\n",
    "\n",
    "    def _save_metrics(metrics_dict):\n",
    "        if metrics_path is None:\n",
    "            return\n",
    "        metrics_to_save = dict(metrics_dict)\n",
    "        metrics_to_save['components'] = np.array(components_list, dtype=int)\n",
    "        metrics_to_save.pop('d_list', None)\n",
    "        metrics_to_save.pop('M_list', None)\n",
    "        np.savez_compressed(\n",
    "            metrics_path,\n",
    "            **metrics_to_save,\n",
    "            d_list=np.array(d_list),\n",
    "            M_list=np.array(M_list),\n",
    "        )\n",
    "        print(f'Saved metrics to {metrics_path}')\n",
    "\n",
    "    def _align_metrics_to_d_list(metrics_dict):\n",
    "        cached_d_list = metrics_dict.get('d_list')\n",
    "        if cached_d_list is None:\n",
    "            return metrics_dict\n",
    "        cached_d_list = [int(v) for v in list(cached_d_list)]\n",
    "        if cached_d_list == list(d_list):\n",
    "            return metrics_dict\n",
    "        dim_map = {dim: idx for idx, dim in enumerate(cached_d_list)}\n",
    "        components_cached = metrics_dict.get('components', [])\n",
    "        components_count = len(list(components_cached))\n",
    "        fill_comp = [np.nan] * components_count\n",
    "        metrics_dict['d_list'] = list(d_list)\n",
    "\n",
    "        def _remap_list(values, fill_value):\n",
    "            values = list(values) if values is not None else []\n",
    "            remapped = [fill_value] * len(d_list)\n",
    "            for new_idx, dim in enumerate(d_list):\n",
    "                old_idx = dim_map.get(dim)\n",
    "                if old_idx is None or old_idx >= len(values):\n",
    "                    continue\n",
    "                remapped[new_idx] = values[old_idx]\n",
    "            return remapped\n",
    "\n",
    "        def _remap_by_M(metrics_by_M, fill_value):\n",
    "            metrics_by_M = metrics_by_M or {}\n",
    "            remapped = {}\n",
    "            for M, series in metrics_by_M.items():\n",
    "                remapped[M] = _remap_list(series, fill_value)\n",
    "            return remapped\n",
    "\n",
    "        metrics_dict['ess_by_M'] = _remap_by_M(metrics_dict.get('ess_by_M'), np.nan)\n",
    "        metrics_dict['msjd_by_M'] = _remap_by_M(metrics_dict.get('msjd_by_M'), np.nan)\n",
    "        metrics_dict['ess_by_M_components'] = _remap_by_M(\n",
    "            metrics_dict.get('ess_by_M_components'),\n",
    "            fill_comp,\n",
    "        )\n",
    "        metrics_dict['msjd_by_M_components'] = _remap_by_M(\n",
    "            metrics_dict.get('msjd_by_M_components'),\n",
    "            fill_comp,\n",
    "        )\n",
    "        metrics_dict['ess_mh'] = _remap_list(metrics_dict.get('ess_mh'), np.nan)\n",
    "        metrics_dict['msjd_mh'] = _remap_list(metrics_dict.get('msjd_mh'), np.nan)\n",
    "        metrics_dict['ess_mh_components'] = _remap_list(\n",
    "            metrics_dict.get('ess_mh_components'),\n",
    "            fill_comp,\n",
    "        )\n",
    "        metrics_dict['msjd_mh_components'] = _remap_list(\n",
    "            metrics_dict.get('msjd_mh_components'),\n",
    "            fill_comp,\n",
    "        )\n",
    "        return metrics_dict\n",
    "\n",
    "    metrics = None\n",
    "    did_update = False\n",
    "    components_list = None\n",
    "    if metrics_path is not None and metrics_path.exists() and not force_recompute:\n",
    "        print(f'Using cached metrics at {metrics_path}')\n",
    "        cached = dict(np.load(metrics_path, allow_pickle=True))\n",
    "        metrics = {k: _unwrap_loaded(v) for k, v in cached.items()}\n",
    "        if components is not None and 'components' in metrics:\n",
    "            cached_components = [int(v) for v in list(metrics['components'])]\n",
    "            if list(components) != cached_components:\n",
    "                print('Cached metrics components do not match; recomputing.')\n",
    "                metrics = None\n",
    "        if metrics is not None:\n",
    "            metrics = _align_metrics_to_d_list(metrics)\n",
    "\n",
    "    if metrics is None:\n",
    "        # Initialize metrics from scratch.\n",
    "        components_list = _normalize_components(components, dim=max(d_list))\n",
    "        components_count = len(components_list)\n",
    "        metrics = {\n",
    "            'components': components_list,\n",
    "            'ess_by_M': {M: [] for M in M_list},\n",
    "            'msjd_by_M': {M: [] for M in M_list},\n",
    "            'ess_by_M_components': {M: [] for M in M_list},\n",
    "            'msjd_by_M_components': {M: [] for M in M_list},\n",
    "            'ess_mh': [],\n",
    "            'msjd_mh': [],\n",
    "            'ess_mh_components': [],\n",
    "            'msjd_mh_components': [],\n",
    "        }\n",
    "        for d_idx, d_cur in enumerate(d_list):\n",
    "            for M in M_list:\n",
    "                chain = load_chain(output_dir, d_cur, 'mess', M=M)\n",
    "                chain = _apply_burnin(chain)\n",
    "                if chain is None:\n",
    "                    metrics['ess_by_M'][M].append(np.nan)\n",
    "                    metrics['msjd_by_M'][M].append(np.nan)\n",
    "                    metrics['ess_by_M_components'][M].append([np.nan] * components_count)\n",
    "                    metrics['msjd_by_M_components'][M].append([np.nan] * components_count)\n",
    "                    continue\n",
    "                selected_ess, selected_msjd = _compute_from_chain(chain, components_list)\n",
    "                metrics['ess_by_M'][M].append(_mean_selected(selected_ess))\n",
    "                metrics['msjd_by_M'][M].append(_mean_selected(selected_msjd))\n",
    "                metrics['ess_by_M_components'][M].append(selected_ess)\n",
    "                metrics['msjd_by_M_components'][M].append(selected_msjd)\n",
    "                did_update = True\n",
    "                if save_every_update:\n",
    "                    _save_metrics(metrics)\n",
    "\n",
    "            mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "            chain_mh = load_chain(\n",
    "                output_dir,\n",
    "                d_cur,\n",
    "                'mh',\n",
    "                proposal_std=mh_std,\n",
    "                proposal_cov=mh_proposal_cov,\n",
    "            )\n",
    "            chain_mh = _apply_burnin(chain_mh)\n",
    "            if chain_mh is None:\n",
    "                metrics['ess_mh'].append(np.nan)\n",
    "                metrics['msjd_mh'].append(np.nan)\n",
    "                metrics['ess_mh_components'].append([np.nan] * components_count)\n",
    "                metrics['msjd_mh_components'].append([np.nan] * components_count)\n",
    "            else:\n",
    "                selected_ess, selected_msjd = _compute_from_chain(chain_mh, components_list)\n",
    "                metrics['ess_mh'].append(_mean_selected(selected_ess))\n",
    "                metrics['msjd_mh'].append(_mean_selected(selected_msjd))\n",
    "                metrics['ess_mh_components'].append(selected_ess)\n",
    "                metrics['msjd_mh_components'].append(selected_msjd)\n",
    "                did_update = True\n",
    "                if save_every_update:\n",
    "                    _save_metrics(metrics)\n",
    "\n",
    "        return metrics, did_update\n",
    "\n",
    "    components_list = [int(v) for v in list(metrics.get('components', []))]\n",
    "    if not components_list:\n",
    "        components_list = _normalize_components(components, dim=max(d_list))\n",
    "        metrics['components'] = components_list\n",
    "    components_count = len(components_list)\n",
    "\n",
    "    metrics['ess_by_M'] = _ensure_by_M(metrics.get('ess_by_M'))\n",
    "    metrics['msjd_by_M'] = _ensure_by_M(metrics.get('msjd_by_M'))\n",
    "    metrics['ess_by_M_components'] = _ensure_by_M_components(\n",
    "        metrics.get('ess_by_M_components'),\n",
    "        components_count,\n",
    "    )\n",
    "    metrics['msjd_by_M_components'] = _ensure_by_M_components(\n",
    "        metrics.get('msjd_by_M_components'),\n",
    "        components_count,\n",
    "    )\n",
    "    metrics['ess_mh'] = _ensure_list(metrics.get('ess_mh', []), len(d_list))\n",
    "    metrics['msjd_mh'] = _ensure_list(metrics.get('msjd_mh', []), len(d_list))\n",
    "    metrics['ess_mh_components'] = _ensure_list(\n",
    "        metrics.get('ess_mh_components', []),\n",
    "        len(d_list),\n",
    "        fill_value=[np.nan] * components_count,\n",
    "    )\n",
    "    metrics['msjd_mh_components'] = _ensure_list(\n",
    "        metrics.get('msjd_mh_components', []),\n",
    "        len(d_list),\n",
    "        fill_value=[np.nan] * components_count,\n",
    "    )\n",
    "\n",
    "    for d_idx, d_cur in enumerate(d_list):\n",
    "        for M in M_list:\n",
    "            if any([\n",
    "                _is_missing(metrics['ess_by_M'][M][d_idx]),\n",
    "                _is_missing(metrics['msjd_by_M'][M][d_idx]),\n",
    "            ]):\n",
    "                chain = load_chain(output_dir, d_cur, 'mess', M=M)\n",
    "                chain = _apply_burnin(chain)\n",
    "                if chain is None:\n",
    "                    continue\n",
    "                selected_ess, selected_msjd = _compute_from_chain(chain, components_list)\n",
    "                metrics['ess_by_M'][M][d_idx] = _mean_selected(selected_ess)\n",
    "                metrics['msjd_by_M'][M][d_idx] = _mean_selected(selected_msjd)\n",
    "                metrics['ess_by_M_components'][M][d_idx] = selected_ess\n",
    "                metrics['msjd_by_M_components'][M][d_idx] = selected_msjd\n",
    "                did_update = True\n",
    "                if save_every_update:\n",
    "                    _save_metrics(metrics)\n",
    "\n",
    "        if any([\n",
    "            _is_missing(metrics['ess_mh'][d_idx]),\n",
    "            _is_missing(metrics['msjd_mh'][d_idx]),\n",
    "        ]):\n",
    "            mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "            chain_mh = load_chain(\n",
    "                output_dir,\n",
    "                d_cur,\n",
    "                'mh',\n",
    "                proposal_std=mh_std,\n",
    "                proposal_cov=mh_proposal_cov,\n",
    "            )\n",
    "            chain_mh = _apply_burnin(chain_mh)\n",
    "            if chain_mh is None:\n",
    "                continue\n",
    "            selected_ess, selected_msjd = _compute_from_chain(chain_mh, components_list)\n",
    "            metrics['ess_mh'][d_idx] = _mean_selected(selected_ess)\n",
    "            metrics['msjd_mh'][d_idx] = _mean_selected(selected_msjd)\n",
    "            metrics['ess_mh_components'][d_idx] = selected_ess\n",
    "            metrics['msjd_mh_components'][d_idx] = selected_msjd\n",
    "            did_update = True\n",
    "            if save_every_update:\n",
    "                _save_metrics(metrics)\n",
    "\n",
    "    return metrics, did_update\n",
    "\n",
    "recompute_metrics = False\n",
    "metrics_path = reports_dir / 'effss_msjd.npz'\n",
    "components = [0, 1, 2, 3, 9, 10, 16, 17]  # Indices into a_true to summarize.\n",
    "metrics, metrics_updated = compute_metrics_by_dim(\n",
    "    burnin=10000,\n",
    "    max_lag=1500,\n",
    "    components=components,\n",
    "    metrics_path=metrics_path,\n",
    "    force_recompute=recompute_metrics,\n",
    " )\n",
    "if metrics_path.exists() and not recompute_metrics and not metrics_updated:\n",
    "    print(f'Metrics file already exists, not overwriting: {metrics_path}')\n",
    "else:\n",
    "    metrics_to_save = dict(metrics)\n",
    "    metrics_to_save['components'] = np.array(metrics.get('components', components), dtype=int)\n",
    "    metrics_to_save.pop('d_list', None)\n",
    "    metrics_to_save.pop('M_list', None)\n",
    "    np.savez_compressed(\n",
    "        metrics_path,\n",
    "        **metrics_to_save,\n",
    "        d_list=np.array(d_list),\n",
    "        M_list=np.array(M_list),\n",
    "    )\n",
    "    print(f'Saved metrics to {metrics_path}')\n",
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ea762",
   "metadata": {},
   "source": [
    "## Plot ESS/MSJD vs Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b213e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ess_msjd_vs_dim(\n",
    "    ess_by_M, ess_mh, msjd_by_M, msjd_mh, ylabel_ess, ylabel_msjd, filename, plot_dir, yscale=None,\n",
    "    ):\n",
    "    font_size = 24\n",
    "    tick_size = 20\n",
    "    legend_font_size = 24\n",
    "    marker_size = 10\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14.5, 6.5), sharex=True)\n",
    "    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(M_list)))\n",
    "\n",
    "    for M, color in zip(M_list, colors):\n",
    "        axes[0].plot(\n",
    "            d_list, ess_by_M[M], marker='o', markersize=marker_size, color=color, label=f'M={M}'\n",
    "        )\n",
    "        axes[1].plot(\n",
    "            d_list, msjd_by_M[M], marker='o', markersize=marker_size, color=color, label=f'M={M}'\n",
    "        )\n",
    "\n",
    "    axes[0].plot(\n",
    "        d_list, ess_mh, marker='s', markersize=marker_size, color='black', linestyle='--', label='MH'\n",
    "    )\n",
    "    axes[1].plot(\n",
    "        d_list, msjd_mh, marker='s', markersize=marker_size, color='black', linestyle='--', label='MH'\n",
    "    )\n",
    "\n",
    "    ess_label = ylabel_ess\n",
    "    msjd_label = ylabel_msjd\n",
    "    if yscale:\n",
    "        ess_label = f\"{ylabel_ess} (log)\"\n",
    "        msjd_label = f\"{ylabel_msjd} (log)\"\n",
    "        axes[0].set_yscale(yscale)\n",
    "        axes[1].set_yscale(yscale)\n",
    "\n",
    "    axes[0].set_xlabel('d', fontsize=font_size)\n",
    "    axes[1].set_xlabel('d', fontsize=font_size)\n",
    "    axes[0].set_ylabel(ess_label, fontsize=font_size)\n",
    "    axes[1].set_ylabel(msjd_label, fontsize=font_size)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[0].tick_params(axis='both', labelsize=tick_size)\n",
    "    axes[1].tick_params(axis='both', labelsize=tick_size)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    legend_ncol = min(len(labels), 6)\n",
    "    fig.legend(\n",
    "        handles, labels,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.02),\n",
    "        ncol=legend_ncol,\n",
    "        frameon=False,\n",
    "        fontsize=legend_font_size,\n",
    "        columnspacing=1.2,\n",
    "        handlelength=1.6,\n",
    "        handletextpad=0.4,\n",
    "    )\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.92))\n",
    "    plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(plot_dir / filename, dpi=300)\n",
    "    return fig\n",
    "\n",
    "def scale_metric_by_M(metric_by_M, scale):\n",
    "    return {\n",
    "        M: (np.array(values, dtype=float) / scale).tolist()\n",
    "        for M, values in metric_by_M.items()\n",
    "    }\n",
    "\n",
    "def scale_metric_list(metric_list, scale):\n",
    "    return (np.array(metric_list, dtype=float) / scale).tolist()\n",
    "\n",
    "def _component_series(metric_by_M_components, metric_mh_components, comp_pos):\n",
    "    by_M = {}\n",
    "    for M in M_list:\n",
    "        series = []\n",
    "        for row in metric_by_M_components.get(M, []):\n",
    "            if row is None or len(row) <= comp_pos:\n",
    "                series.append(np.nan)\n",
    "            else:\n",
    "                series.append(row[comp_pos])\n",
    "        by_M[M] = series\n",
    "    mh_series = []\n",
    "    for row in metric_mh_components:\n",
    "        if row is None or len(row) <= comp_pos:\n",
    "            mh_series.append(np.nan)\n",
    "        else:\n",
    "            mh_series.append(row[comp_pos])\n",
    "    return by_M, mh_series\n",
    "\n",
    "ess_ylabel = 'Eff. Sample Size'\n",
    "\n",
    "ess_by_M_scaled = metrics['ess_by_M']\n",
    "ess_mh_scaled = metrics['ess_mh']\n",
    "\n",
    "components_list = [int(v) for v in metrics.get('components', [])]\n",
    "component_positions = list(range(min(5, len(components_list))))\n",
    "component_labels = [f'component {components_list[pos]}' for pos in component_positions]\n",
    "\n",
    "plot_dir = reports_dir / 'ess_msjd_vs_d'\n",
    "\n",
    "# Mean over selected components\n",
    "plot_ess_msjd_vs_dim(\n",
    "    ess_by_M_scaled, ess_mh_scaled,\n",
    "    metrics['msjd_by_M'], metrics['msjd_mh'],\n",
    "    ess_ylabel, 'MSJD', 'ess_msjd_vs_d_mean.png', plot_dir,\n",
    " )\n",
    "plot_ess_msjd_vs_dim(\n",
    "    ess_by_M_scaled, ess_mh_scaled,\n",
    "    metrics['msjd_by_M'], metrics['msjd_mh'],\n",
    "    ess_ylabel, 'MSJD', 'ess_msjd_vs_d_mean_log.png', plot_dir, yscale='log',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-component ESS/MSJD plots\n",
    "if len(component_positions) == 0:\n",
    "    print('No components available for plotting.')\n",
    "else:\n",
    "    for comp_pos, comp_label in zip(component_positions, component_labels):\n",
    "        ess_by_M_comp, ess_mh_comp = _component_series(\n",
    "            metrics['ess_by_M_components'],\n",
    "            metrics['ess_mh_components'],\n",
    "            comp_pos,\n",
    "        )\n",
    "        ess_by_M_comp_scaled = scale_metric_by_M(ess_by_M_comp, 1)\n",
    "        ess_mh_comp_scaled = scale_metric_list(ess_mh_comp, 1)\n",
    "        msjd_by_M_comp, msjd_mh_comp = _component_series(\n",
    "            metrics['msjd_by_M_components'],\n",
    "            metrics['msjd_mh_components'],\n",
    "            comp_pos,\n",
    "        )\n",
    "        filename = f\"ess_msjd_vs_d_{comp_label.replace(' ', '_')}.png\"\n",
    "        plot_ess_msjd_vs_dim(\n",
    "            ess_by_M_comp_scaled, ess_mh_comp_scaled,\n",
    "            msjd_by_M_comp, msjd_mh_comp,\n",
    "            ess_ylabel, 'MSJD', filename, plot_dir,\n",
    "        )\n",
    "        log_filename = f\"ess_msjd_vs_d_{comp_label.replace(' ', '_')}_log.png\"\n",
    "        plot_ess_msjd_vs_dim(\n",
    "            ess_by_M_comp_scaled, ess_mh_comp_scaled,\n",
    "            msjd_by_M_comp, msjd_mh_comp,\n",
    "            ess_ylabel, 'MSJD', log_filename, plot_dir, yscale='log',\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24587c",
   "metadata": {},
   "source": [
    "## Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ba47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior hist grid (make_hist_grid_comps) for a selected chain.\n",
    "from mess.plotting.diagnostics import make_hist_grid_comps\n",
    "\n",
    "def get_component_labels(dim_value, comps):\n",
    "    labels = []\n",
    "    for k in comps:\n",
    "        count = 0\n",
    "        found = False\n",
    "        for i in range(dim_value):\n",
    "            for j in range(i + 1, dim_value):\n",
    "                if count == k:\n",
    "                    labels.append(f\"$a_{{{i}{j}}}$\")\n",
    "                    found = True\n",
    "                    break\n",
    "                count += 1\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            labels.append(f\"$\\\\mathrm{{param}}_{{{k}}}$\")\n",
    "    return labels\n",
    "\n",
    "hist_d = 10\n",
    "hist_alg = 'mess'  # 'mess' or 'mh'\n",
    "hist_M = 100  # only used for MESS\n",
    "hist_burnin = 10000\n",
    "hist_thin = 5\n",
    "hist_params = 6\n",
    "zoom_factor = 0.6\n",
    "share_axes = True\n",
    "font_size = 18\n",
    "tick_label_size = font_size - 2\n",
    "pairplots_dir = reports_dir / \"pairplots\"\n",
    "pairplots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if hist_alg == 'mess':\n",
    "    chain_hist = load_chain(output_dir, hist_d, 'mess', M=hist_M)\n",
    "    hist_label = f'MESS (M={hist_M}), d={hist_d}'\n",
    "elif hist_alg == 'mh':\n",
    "    d_idx = d_list.index(hist_d)\n",
    "    mh_std = mh_proposal_stds_scaled[d_idx]\n",
    "    chain_hist = load_chain(output_dir, hist_d, 'mh', proposal_std=mh_std)\n",
    "    hist_label = f'MH d={hist_d} sigma2={mh_std}'\n",
    "else:\n",
    "    raise ValueError(\"hist_alg must be 'mess' or 'mh'\")\n",
    "\n",
    "if chain_hist is None or chain_hist.size == 0:\n",
    "    print(f'No chain found for {hist_label}')\n",
    "else:\n",
    "    post = chain_hist[hist_burnin::hist_thin]\n",
    "    n_params = min(hist_params, post.shape[1])\n",
    "    comp_list = np.arange(n_params)\n",
    "    comp_list10 = [0, 1, 2, 3, 9, 10]\n",
    "    comp_list10_short = [0, 1, 9]\n",
    "    comp_list20 = [0, 1, 2, 3, 19, 20]\n",
    "    comp_list50 = [0, 1, 2, 3, 49, 50]\n",
    "    comp_list = comp_list10_short\n",
    "    n_params = len(comp_list)\n",
    "    label_map = dict(zip(comp_list, get_component_labels(hist_d, comp_list)))\n",
    "    data_hist = get_dataset_for_dim(hist_d, seed=seed_data)\n",
    "    prior_diag = data_hist['prior_diag']\n",
    "    C = np.diag(prior_diag)\n",
    "    vals = post[:, comp_list]\n",
    "    max_abs = float(np.max(np.abs(vals))) if vals.size else 1.0\n",
    "    true_vals = data_hist['a_true']\n",
    "    max_abs = max(max_abs, float(np.max(np.abs(true_vals[comp_list]))))\n",
    "    if max_abs == 0.0:\n",
    "        max_abs = 1.0\n",
    "    R_full = 0.9 * max_abs\n",
    "    dr = R_full / 60.0\n",
    "    M_tag = hist_M if hist_alg == 'mess' else 'NA'\n",
    "    filename = f\"pairplot_alg{hist_alg}_d{hist_d}_M{M_tag}_n{n_params}.png\"\n",
    "    fig = make_hist_grid_comps(\n",
    "        R_full,\n",
    "        dr,\n",
    "        post,\n",
    "        comp_list,\n",
    "        save_path=pairplots_dir / filename,\n",
    "        C=C,\n",
    "        beta=0.95,\n",
    "        hide_plot=False,\n",
    "        label_map=label_map,\n",
    "        font_size=font_size+8,\n",
    "        title='', #f'{hist_label}',\n",
    "        figsize=(12, 12),\n",
    "        true_values=data_hist['a_true'],\n",
    "        show_ellipses=False\n",
    "    )\n",
    "    for ax in fig.axes:\n",
    "        ax.tick_params(axis='both', labelsize=tick_label_size)\n",
    "    if share_axes and zoom_factor and 0 < zoom_factor < 1:\n",
    "        R_zoom = zoom_factor * R_full\n",
    "        axes = np.array(fig.axes).reshape(n_params, n_params)\n",
    "        for i in range(n_params):\n",
    "            for j in range(n_params):\n",
    "                ax = axes[i, j]\n",
    "                ax.set_xlim([-R_zoom, R_zoom])\n",
    "                if i != j:\n",
    "                    ax.set_ylim([-R_zoom, R_zoom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f0455f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mess-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
